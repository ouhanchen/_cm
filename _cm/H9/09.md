一、 核心觀念解析1. 線性與代數線性 (Linear)： 指的是滿足「疊加原理」的特性，包含兩點：外加性 $f(x+y) = f(x) + f(y)$ 與 齊次性 $f(cx) = c f(x)$。幾何上，線性變換會保持網格平行且等距，且原點不動。代數 (Algebra)： 意指「用符號代表數」。在線性代數中，我們將整個向量或矩陣視為單一符號進行運算，這讓我們能用簡潔的方程式描述複雜的多維系統。2. 空間與向量空間空間 (Space)： 在數學中，空間是指一個集合，且該集合內的元素滿足特定的運算規則。向量空間 (Vector Space)： 之所以稱為空間，是因為它定義了一個「環境」，在這個環境中的任何向量經過加法或係數乘法後，結果仍然會落在這個環境內（封閉性）。3. 矩陣與向量的關係關係： 向量是「狀態」（空間中的一個點或方向），而矩陣是「動作」（對向量進行轉換）。矩陣的意義： 矩陣代表一個 線性映射 (Linear Transformation)。當你用矩陣 $A$ 乘以向量 $x$，本質上是將 $x$ 從一個座標系轉換到另一個座標系。4. 幾何操作與矩陣在 2D/3D 中，縮放與旋轉可以用 $2 \times 2$ 或 $3 \times 3$ 矩陣表示，但 平移 (Translation) 需要使用 齊次座標 (Homogeneous Coordinates)：縮放： 對角矩陣 $\begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix}$。旋轉 (2D)： $\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$。平移： 必須升維至 $(n+1) \times (n+1)$ 矩陣，例如 2D 平移為 $\begin{bmatrix} 1 & 0 & t_x \\ 0 & 1 & t_y \\ 0 & 0 & 1 \end{bmatrix}$。二、 行列式與分解技術5. 行列式 (Determinant)意義： 行列式代表變換後的 空間縮放比例。體積關係： 在 2D 中，行列式的絕對值是平行四邊形的面積；在 3D 中，則是平行六面體的體積。遞迴計算 (Laplace Expansion)： 透過餘因子展開，將 $n \times n$ 矩陣降階處理。對角化計算： 若 $A = PDP^{-1}$，則 $\det(A) = \det(P)\det(D)\det(P^{-1}) = \det(D)$，即所有特徵值的乘積。LU 分解計算： $A = LU$，則 $\det(A) = \det(L) \times \det(U)$。由於 $L$ 對角線全為 1，$U$ 為上三角，故行列式即為 $U$ 對角線元素的乘積。6. 特徵值分解 (Eigendecomposition)意義： 尋找變換中「方向不變」的向量（特徵向量）以及其「縮放倍率」（特徵值）。用途： 簡化矩陣運算（如計算矩陣的次方 $A^k$）、數據降維、解微分方程。7. QR 分解與特徵值疊代QR 分解： 將矩陣分解為正交矩陣 $Q$ 與上三角矩陣 $R$。QR 演算法： 不斷進行 $A_k = Q_k R_k$ 然後令 $A_{k+1} = R_k Q_k$。隨著疊代，矩陣會趨於上三角陣，其對角線即為特徵值。8. SVD (奇異值分解) 與 PCASVD： $A = U\Sigma V^T$。適用於任何矩陣（不限方陣）。它將變換分解為：旋轉 $\rightarrow$ 拉伸 $\rightarrow$ 旋轉。PCA (主成分分析)： 是一種降維技術。關係： PCA 的核心是對數據的共變異數矩陣做特徵值分解，這等同於對去中心化後的數據矩陣做 SVD。SVD 的右奇異向量 $V$ 就是 PCA 的主成分方向。